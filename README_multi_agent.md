# ğŸš—ğŸš— CARLA Lane Keeping â€” Multi-Agent PPO

> **MSc Thesis Project** | Multi-agent extension of a PPO-based lane-keeping system, training multiple independent vehicles simultaneously inside the CARLA simulator.

---

## ğŸ“Œ Overview

This project extends the [single-agent lane-keeping system](https://github.com/YOUR_USERNAME/carla-lane-keeping-ppo-single-agent) to a **multi-agent setting**, where multiple vehicles are trained in parallel within the same CARLA world. Each agent operates independently (no inter-agent coordination), sharing the environment but learning from its own experience stream â€” effectively providing richer and more diverse training data to the PPO policy.

The work is part of an MSc thesis exploring deep reinforcement learning for autonomous vehicle control.

---

## ğŸ¬ Demo

> *(Video clip of multiple agents performing lane keeping simultaneously)*

<!-- Replace with your actual GIF/video link after uploading -->
<!-- ![Multi-Agent Demo](assets/demo.gif) -->

ğŸ“¹ A ~30-second clip of agents executing smooth lane-keeping maneuvers is available in the `assets/` folder.

---

## âœ¨ Key Features

- **Multiple independent agents** running in the same CARLA world simultaneously
- **Custom `VecEnv` wrapper** (`MultiAgentCarlaWrapper`) compatible with Stable-Baselines3
- **Ghost mode**: agents do not collide with each other (physically independent)
- All single-agent features are preserved:
  - Stacked-frame visual observations (4 Ã— 128Ã—128 RGB)
  - Continuous action space: steering, throttle, brake
  - Shaped reward function with collision, lane departure, and smoothness terms
  - PPO with linear LR schedule and entropy annealing
  - TensorBoard logging and model checkpointing

---

## ğŸ—‚ï¸ Project Structure

```
carla-lane-keeping-ppo-multi-agent/
â”‚
â”œâ”€â”€ carla_lane_keeping_env.py      # Custom CARLA Gymnasium environment (single agent)
â”œâ”€â”€ multi_agent_wrapper.py         # VecEnv wrapper for multi-agent training
â”œâ”€â”€ training_lane_keeping_ppo.py   # PPO training script (multi-agent enabled)
â”œâ”€â”€ lane_keeping_parameters.py     # All hyperparameters including MultiAgentParams
â”œâ”€â”€ assets/                        # Demo videos / plots (add manually)
â””â”€â”€ README.md
```

---

## âš™ï¸ Multi-Agent Architecture

The `MultiAgentCarlaWrapper` inherits from Stable-Baselines3's `VecEnv`, making it a drop-in replacement for vectorized environments. Each agent is a full `CarlaLaneKeepingEnv` instance with its own vehicle spawned in the CARLA world.

```
PPO Policy
    â”‚
    â–¼
MultiAgentCarlaWrapper (VecEnv)
    â”œâ”€â”€ CarlaLaneKeepingEnv  â”€â”€â–º Vehicle 1  (CARLA World)
    â”œâ”€â”€ CarlaLaneKeepingEnv  â”€â”€â–º Vehicle 2  (CARLA World)
    â””â”€â”€ CarlaLaneKeepingEnv  â”€â”€â–º Vehicle N  (CARLA World)
```

### Multi-Agent Parameters

| Parameter | Value |
|-----------|-------|
| Number of Agents | 2 (configurable) |
| Agent Interaction | Disabled (ghost mode) |
| Spawn Strategy | Random, spaced 50m apart |
| Sync Mode | Synchronous (stable) |

---

## âš™ï¸ Environment Details

| Parameter | Value |
|-----------|-------|
| Simulator | CARLA 0.9.15 |
| Map | Town04 |
| Target Speed | 8.0 m/s (~30 km/h) |
| Max Episode Steps | 2000 |
| Observation | 4 Ã— 128Ã—128 RGB frames + 6D vehicle state vector |
| Action Space | Continuous: `[steering, throttle, brake]` |
| Framework | Stable-Baselines3 (PPO) |

### Reward Function Summary

| Component | Weight |
|-----------|--------|
| Lane center distance | âˆ’1.0 |
| Forward progress | +20.0 |
| Speed maintenance | +5.0 |
| Orientation error | âˆ’0.5 |
| Collision | âˆ’50.0 |
| Lane departure | âˆ’20.0 |
| Action smoothness | âˆ’0.1 / âˆ’0.05 |
| Episode completion bonus | +60.0 |

---

## ğŸ§  PPO Hyperparameters

| Parameter | Value |
|-----------|-------|
| Learning Rate | 5e-4 (linear schedule) |
| Clip Range | 0.2 |
| Entropy Coeff | 0.02 â†’ 0.005 (annealed) |
| GAE Lambda | 0.95 |
| Discount (Î³) | 0.99 |
| N Steps | 4096 |
| Batch Size | 512 |
| Epochs per Update | 4 |
| Network | [256, 256] + tanh |
| Total Timesteps | 200,000 |

---

## ğŸ› ï¸ Requirements

- Windows 10/11 (CARLA runs natively on Windows)
- Python 3.7
- CARLA 0.9.15
- CUDA-capable GPU (recommended â€” multiple agents are GPU-intensive)
- Sufficient RAM (â‰¥ 16 GB recommended for 2+ agents)

### Python Dependencies

```bash
pip install stable-baselines3[extra]
pip install gymnasium
pip install numpy
pip install tensorboard
```

---

## ğŸš€ Getting Started

### 1. Install CARLA

Download [CARLA 0.9.15](https://github.com/carla-simulator/carla/releases/tag/0.9.15) and extract it.

### 2. Configure Paths and Agent Count

Open `lane_keeping_parameters.py` and update:

```python
# Paths
BASE_PATH = "your\\path\\to\\project"
CARLA_EGG_PATH = "your\\path\\to\\carla-0.9.15-py3.7-win-amd64.egg"

# Multi-agent settings
class MultiAgentParams:
    ENABLE_MULTI_AGENT = True
    NUM_AGENTS = 2          # Increase if your hardware supports it
```

### 3. Launch CARLA Server

```bash
cd "C:\path\to\CARLA_0.9.15\WindowsNoEditor"
CarlaUE4.exe -quality-level=Low -benchmark -fps=20
```

> âš ï¸ **Important:** Use `-quality-level=Low` when running multiple agents to reduce GPU load.

### 4. Run Training

```bash
python training_lane_keeping_ppo.py
```

The wrapper will automatically spawn all agents with a 2-second delay between each to avoid CARLA spawn conflicts.

### 5. Monitor Training (Optional)

```bash
tensorboard --logdir ./logs
```

---

## ğŸ“Š Training Notes

- Each agent auto-resets independently when its episode ends
- Failed agent steps are handled gracefully with dummy observations â€” training continues uninterrupted
- Models are saved every **10,000 timesteps**
- Evaluation runs every **5,000 timesteps** over 5 episodes
- To add more agents, increase `MultiAgentParams.NUM_AGENTS` â€” but monitor your VRAM and RAM usage

---

## âš¡ Single-Agent vs Multi-Agent Comparison

| Feature | Single Agent | Multi Agent |
|---------|-------------|-------------|
| Vehicles in world | 1 | 2+ |
| Experience diversity | Lower | Higher |
| Training speed | Baseline | Faster (parallel data) |
| Hardware requirement | Moderate | Higher |
| VecEnv compatible | No | Yes |

---

## ğŸ”— Related Project

This is the **multi-agent** extension. The original **single-agent** version is available here:

ğŸ‘‰ [carla-lane-keeping-ppo-single-agent](https://github.com/YOUR_USERNAME/carla-lane-keeping-ppo-single-agent)

---

## ğŸ“ Citation / Reference

```
[Your Name] (2025). CARLA Lane Keeping with PPO â€” Multi-Agent.
MSc Thesis Project. GitHub: https://github.com/YOUR_USERNAME/carla-lane-keeping-ppo-multi-agent
```

---

## ğŸ“¬ Contact

Feel free to open an issue or reach out if you have questions about the implementation.
